{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06392057-30e3-4199-afc5-7f5b55a4a210",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/generated_answers.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../results/generated_answers.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../results/generated_answers.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/generated_answers.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3e7fb2b-4c5f-4b8c-98b6-a66c5ec9c298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrida\\llm-hallucination-project\\notebook\n",
      "['.ipynb_checkpoints', '01_load_dataset.ipynb', 'data', 'evaluation', 'notebook', 'results', 'retrieval', 'Untitled.ipynb', 'verification']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "print(os.listdir(\"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbac69fb-2909-4f49-99b1-b45cd21194f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"../results\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f44779c1-cd88-4ef9-acd9-a7a9c07a2ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/final_scored.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../results/final_scored.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../results/final_scored.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../results/final_scored.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a82ae29-f0ce-488f-aa89-331066736ae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/final_scored.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../results/final_scored.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../results/final_scored.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../results/final_scored.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9d2316f-ec62-4c28-9624-851a168d497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current folder: C:\\Users\\hrida\\llm-hallucination-project\\notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current folder:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dea9c69-9f81-4b6a-a441-86f4a5cdc3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '02_build_retriever.ipynb',\n",
       " '03_generate_answers.ipynb',\n",
       " '4_hallucination_scoring.ipynb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd76371-03de-46c6-a8bb-fcb8895f632a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '01_load_dataset.ipynb',\n",
       " 'data',\n",
       " 'evaluation',\n",
       " 'notebook',\n",
       " 'results',\n",
       " 'retrieval',\n",
       " 'Untitled.ipynb',\n",
       " 'verification']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "192d80c6-e3c2-4885-9baf-587bd29c9c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f03ff706-9d1b-4056-a706-149a1c0bbd25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_small' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_small\u001b[49m.to_csv(\u001b[33m\"\u001b[39m\u001b[33m../results/generated_answers.csv\u001b[39m\u001b[33m\"\u001b[39m, ...)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_small' is not defined"
     ]
    }
   ],
   "source": [
    "df_small.to_csv(\"../results/generated_answers.csv\", ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5491a87-432e-40bb-8602-c6e4bb3039f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m df = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m../data/fever_small.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# recreate df_small from first N answers\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m N = \u001b[38;5;28mlen\u001b[39m(\u001b[43manswers\u001b[49m)\n\u001b[32m      9\u001b[39m df_small = df.head(N).copy()\n\u001b[32m     10\u001b[39m df_small[\u001b[33m\"\u001b[39m\u001b[33mmodel_answer\u001b[39m\u001b[33m\"\u001b[39m] = answers\n",
      "\u001b[31mNameError\u001b[39m: name 'answers' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reload main dataframe\n",
    "df = pd.read_csv(\"../data/fever_small.csv\")\n",
    "\n",
    "# recreate df_small from first N answers\n",
    "N = len(answers)\n",
    "\n",
    "df_small = df.head(N).copy()\n",
    "df_small[\"model_answer\"] = answers\n",
    "\n",
    "df_small.to_csv(\"../results/generated_answers.csv\", index=False)\n",
    "\n",
    "print(\"Saved generated_answers.csv to results ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "802cf3dd-c937-4ed1-81c2-63ac75ea7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/fever_small.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da1307a-6e81-4f93-8c0f-7a5877335e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0630cca32a694be28d9f211871bce67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "index = faiss.read_index(\"../retrieval/fever.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db6961c-e4f8-4f83-9ebd-4b97e33f2de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, k=3):\n",
    "    q_emb = embedder.encode([query])\n",
    "    _, indices = index.search(np.array(q_emb), k)\n",
    "    return indices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10e6b463-cf18-4cff-b642-02caa74fa52c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdb958a747040db874e0f8177c6ba51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "405f3577-2ff8-4697-ad04-9e7d5e893a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=2048) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 20 answers ✅\n"
     ]
    }
   ],
   "source": [
    "docs = df[\"evidence\"].astype(str).tolist()\n",
    "\n",
    "answers = []\n",
    "\n",
    "for claim in df[\"claim\"][:20]:\n",
    "    idxs = retrieve_docs(claim)\n",
    "    context = \" \".join([docs[i] for i in idxs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the text below to answer the claim.\n",
    "\n",
    "TEXT:\n",
    "{context}\n",
    "\n",
    "CLAIM:\n",
    "{claim}\n",
    "\n",
    "Answer whether the claim is true or false and explain briefly.\n",
    "\"\"\"\n",
    "\n",
    "    out = generator(prompt, max_new_tokens=120)\n",
    "    answers.append(out[0][\"generated_text\"])\n",
    "\n",
    "print(\"Generated\", len(answers), \"answers ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "747432f8-25ce-4a2e-a345-b1170e40b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved generated_answers.csv ✅\n"
     ]
    }
   ],
   "source": [
    "df_small = df.head(len(answers)).copy()\n",
    "df_small[\"model_answer\"] = answers\n",
    "\n",
    "df_small.to_csv(\"../results/generated_answers.csv\", index=False)\n",
    "\n",
    "print(\"Saved generated_answers.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01f13985-26a2-4859-9a5a-2ff66bd3fb75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generated_answers.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9cb8a302-5e5e-41bb-898a-2c94bc7fc21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer'],\n",
      "      dtype='str')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>original_id</th>\n",
       "      <th>model_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tie Your Mother Down was released in 2007.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Tie_Your_Mother_Down', '8', \"On several occ...</td>\n",
       "      <td>633922aad8cd96c5e3812afebe56cbc4</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>209856</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matteo Renzi is Italian and was born in Florence.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...</td>\n",
       "      <td>612be5782034160ae0b7d1c21539d740</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>172464</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashley Graham was never on a magazine cover.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...</td>\n",
       "      <td>65cc3f98bece69f9d3a724df9cac82d6</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>126982</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luis Fonsi is Puerto Rican.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...</td>\n",
       "      <td>1aa8074ed9e9c0e067ffe4e8549ba980</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>75871</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murda Beatz is a vegan.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Coke_N_Butter', '2', 'The project consists ...</td>\n",
       "      <td>77fd25cfc2a6099095b27aeb11a6d086</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>135082</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim            label  \\\n",
       "0         Tie Your Mother Down was released in 2007.          REFUTES   \n",
       "1  Matteo Renzi is Italian and was born in Florence.  NOT ENOUGH INFO   \n",
       "2       Ashley Graham was never on a magazine cover.          REFUTES   \n",
       "3                        Luis Fonsi is Puerto Rican.         SUPPORTS   \n",
       "4                            Murda Beatz is a vegan.  NOT ENOUGH INFO   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  [['Tie_Your_Mother_Down', '8', \"On several occ...   \n",
       "1  [['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...   \n",
       "2  [['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...   \n",
       "3  [['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...   \n",
       "4  [['Coke_N_Butter', '2', 'The project consists ...   \n",
       "\n",
       "                                 id      verifiable  original_id  \\\n",
       "0  633922aad8cd96c5e3812afebe56cbc4      VERIFIABLE       209856   \n",
       "1  612be5782034160ae0b7d1c21539d740  NOT VERIFIABLE       172464   \n",
       "2  65cc3f98bece69f9d3a724df9cac82d6      VERIFIABLE       126982   \n",
       "3  1aa8074ed9e9c0e067ffe4e8549ba980      VERIFIABLE        75871   \n",
       "4  77fd25cfc2a6099095b27aeb11a6d086  NOT VERIFIABLE       135082   \n",
       "\n",
       "                                        model_answer  \n",
       "0  \\nUse ONLY the text below to answer the claim....  \n",
       "1  \\nUse ONLY the text below to answer the claim....  \n",
       "2  \\nUse ONLY the text below to answer the claim....  \n",
       "3  \\nUse ONLY the text below to answer the claim....  \n",
       "4  \\nUse ONLY the text below to answer the claim....  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/generated_answers.csv\")\n",
    "print(df.columns)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "205baa2b-c443-4dc8-8de5-f9baa6e0bde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744044ef34a945b28fb181326d6e7539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrida\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hrida\\.cache\\huggingface\\hub\\models--roberta-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcddae840a749388b2f14bf8c60750d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3a46114e2654f6a91ff96f7ed3e956d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification LOAD REPORT from: roberta-large-mnli\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "roberta.pooler.dense.bias   | UNEXPECTED |  | \n",
      "roberta.pooler.dense.weight | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7fb843a4de40c791c39bdfe73fd52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc46839ab9e34dc3a2e73d6d591b96fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0872a408734040248a5e4b1743ee6d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c810cbccb58422e9dcbcc0e31a6e89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nli = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"roberta-large-mnli\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0990e681-f479-44b1-9a57-cbe6cb051a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (979 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index 514 is out of bounds for dimension 1 with size 514",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows():\n\u001b[32m      5\u001b[39m     pair = row[\u001b[33m\"\u001b[39m\u001b[33mevidence\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m </s></s> \u001b[39m\u001b[33m\"\u001b[39m + row[\u001b[33m\"\u001b[39m\u001b[33mmodel_answer\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     out = \u001b[43mnli\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpair\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m out[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mENTAILMENT\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      9\u001b[39m         entail_probs.append(out[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:148\u001b[39m, in \u001b[36mTextClassificationPipeline.__call__\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[33;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[32m    115\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m \u001b[33;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    147\u001b[39m inputs = (inputs,)\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[32m    150\u001b[39m _legacy = \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1274\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[32m   1267\u001b[39m         \u001b[38;5;28miter\u001b[39m(\n\u001b[32m   1268\u001b[39m             \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1271\u001b[39m         )\n\u001b[32m   1272\u001b[39m     )\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1281\u001b[39m, in \u001b[36mPipeline.run_single\u001b[39m\u001b[34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[39m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[32m   1280\u001b[39m     model_inputs = \u001b[38;5;28mself\u001b[39m.preprocess(inputs, **preprocess_params)\n\u001b[32m-> \u001b[39m\u001b[32m1281\u001b[39m     model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1282\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.postprocess(model_outputs, **postprocess_params)\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1173\u001b[39m, in \u001b[36mPipeline.forward\u001b[39m\u001b[34m(self, model_inputs, **forward_params)\u001b[39m\n\u001b[32m   1171\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[32m   1172\u001b[39m         model_inputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_inputs, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m         model_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1174\u001b[39m         model_outputs = \u001b[38;5;28mself\u001b[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:179\u001b[39m, in \u001b[36mTextClassificationPipeline._forward\u001b[39m\u001b[34m(self, model_inputs)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect.signature(model_forward).parameters:\n\u001b[32m    178\u001b[39m     model_inputs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:835\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    834\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    837\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:976\u001b[39m, in \u001b[36mRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, **kwargs)\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    959\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    960\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor] | SequenceClassifierOutput:\n\u001b[32m    961\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    962\u001b[39m \u001b[33;03m    token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    963\u001b[39m \u001b[33;03m        Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    974\u001b[39m \u001b[33;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    975\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    985\u001b[39m     sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    986\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:1002\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1000\u001b[39m             outputs = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m   1001\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m         outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1004\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1005\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1006\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1007\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:636\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    633\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_position \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    634\u001b[39m     cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n\u001b[32m--> \u001b[39m\u001b[32m636\u001b[39m embedding_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    642\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m attention_mask, encoder_attention_mask = \u001b[38;5;28mself\u001b[39m._create_attention_masks(\n\u001b[32m    645\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    646\u001b[39m     encoder_attention_mask=encoder_attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    650\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    651\u001b[39m )\n\u001b[32m    653\u001b[39m encoder_outputs = \u001b[38;5;28mself\u001b[39m.encoder(\n\u001b[32m    654\u001b[39m     embedding_output,\n\u001b[32m    655\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    662\u001b[39m     **kwargs,\n\u001b[32m    663\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:109\u001b[39m, in \u001b[36mRobertaEmbeddings.forward\u001b[39m\u001b[34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\u001b[39;00m\n\u001b[32m    108\u001b[39m     buffered_token_type_ids = \u001b[38;5;28mself\u001b[39m.token_type_ids.expand(position_ids.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     buffered_token_type_ids = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffered_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: index 514 is out of bounds for dimension 1 with size 514"
     ]
    }
   ],
   "source": [
    "entail_probs = []\n",
    "contra_probs = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    pair = row[\"evidence\"] + \" </s></s> \" + row[\"model_answer\"]\n",
    "    out = nli(pair)[0]\n",
    "\n",
    "    if out[\"label\"] == \"ENTAILMENT\":\n",
    "        entail_probs.append(out[\"score\"])\n",
    "        contra_probs.append(0.0)\n",
    "    elif out[\"label\"] == \"CONTRADICTION\":\n",
    "        entail_probs.append(0.0)\n",
    "        contra_probs.append(out[\"score\"])\n",
    "    else:\n",
    "        entail_probs.append(0.0)\n",
    "        contra_probs.append(0.0)\n",
    "\n",
    "df[\"entail_prob\"] = entail_probs\n",
    "df[\"contradict_prob\"] = contra_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b2f2b91-ca1d-4653-94b6-d001336a014e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI scoring done safely ✅\n"
     ]
    }
   ],
   "source": [
    "entail_probs = []\n",
    "contra_probs = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    pair = row[\"evidence\"] + \" </s></s> \" + row[\"model_answer\"]\n",
    "\n",
    "    out = nli(\n",
    "        pair,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )[0]\n",
    "\n",
    "    if out[\"label\"] == \"ENTAILMENT\":\n",
    "        entail_probs.append(out[\"score\"])\n",
    "        contra_probs.append(0.0)\n",
    "    elif out[\"label\"] == \"CONTRADICTION\":\n",
    "        entail_probs.append(0.0)\n",
    "        contra_probs.append(out[\"score\"])\n",
    "    else:\n",
    "        entail_probs.append(0.0)\n",
    "        contra_probs.append(0.0)\n",
    "\n",
    "df[\"entail_prob\"] = entail_probs\n",
    "df[\"contradict_prob\"] = contra_probs\n",
    "\n",
    "print(\"NLI scoring done safely ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0417aba8-9453-4991-9e67-1a7c9b7a12c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../results/final_scored.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../results/final_scored.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m df.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../results/final_scored.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/final_scored.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38c33858-6f52-4ffa-a0e0-5c3938bbabcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current folder: C:\\Users\\hrida\\llm-hallucination-project\\notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current folder:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efc5312f-0f76-4ce9-a910-7b41ad3286c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '02_build_retriever.ipynb',\n",
       " '03_generate_answers.ipynb',\n",
       " '4_hallucination_scoring.ipynb']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f903db83-15f9-4a0a-98f1-d394b6b3c506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " '01_load_dataset.ipynb',\n",
       " 'data',\n",
       " 'evaluation',\n",
       " 'notebook',\n",
       " 'results',\n",
       " 'retrieval',\n",
       " 'Untitled.ipynb',\n",
       " 'verification']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a71ef4a0-2312-460b-b036-9d2862f2c444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['generated_answers.csv']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69ebde80-7531-477a-b212-72cf6518ffca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_scored' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mfinal_scored\u001b[49m.csv\n",
      "\u001b[31mNameError\u001b[39m: name 'final_scored' is not defined"
     ]
    }
   ],
   "source": [
    "final_scored.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98551d26-956a-4c2e-93db-bc2341dcdf02",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/final_scored.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresults/final_scored.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'results/final_scored.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"results/final_scored.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5016c744-b394-4fc1-b9ca-29c10ad4c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final_scored.csv to ../results ✅\n",
      "['final_scored.csv', 'generated_answers.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# make sure results folder exists\n",
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "\n",
    "# save\n",
    "df.to_csv(\"../results/final_scored.csv\", index=False)\n",
    "\n",
    "print(\"Saved final_scored.csv to ../results ✅\")\n",
    "\n",
    "# verify immediately\n",
    "print(os.listdir(\"../results\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d04e0bb0-aaa0-4e02-ba50-dc9b6f101267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>original_id</th>\n",
       "      <th>model_answer</th>\n",
       "      <th>entail_prob</th>\n",
       "      <th>contradict_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tie Your Mother Down was released in 2007.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Tie_Your_Mother_Down', '8', \"On several occ...</td>\n",
       "      <td>633922aad8cd96c5e3812afebe56cbc4</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>209856</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.464638</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matteo Renzi is Italian and was born in Florence.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...</td>\n",
       "      <td>612be5782034160ae0b7d1c21539d740</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>172464</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashley Graham was never on a magazine cover.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...</td>\n",
       "      <td>65cc3f98bece69f9d3a724df9cac82d6</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>126982</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luis Fonsi is Puerto Rican.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...</td>\n",
       "      <td>1aa8074ed9e9c0e067ffe4e8549ba980</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>75871</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murda Beatz is a vegan.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Coke_N_Butter', '2', 'The project consists ...</td>\n",
       "      <td>77fd25cfc2a6099095b27aeb11a6d086</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>135082</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim            label  \\\n",
       "0         Tie Your Mother Down was released in 2007.          REFUTES   \n",
       "1  Matteo Renzi is Italian and was born in Florence.  NOT ENOUGH INFO   \n",
       "2       Ashley Graham was never on a magazine cover.          REFUTES   \n",
       "3                        Luis Fonsi is Puerto Rican.         SUPPORTS   \n",
       "4                            Murda Beatz is a vegan.  NOT ENOUGH INFO   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  [['Tie_Your_Mother_Down', '8', \"On several occ...   \n",
       "1  [['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...   \n",
       "2  [['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...   \n",
       "3  [['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...   \n",
       "4  [['Coke_N_Butter', '2', 'The project consists ...   \n",
       "\n",
       "                                 id      verifiable  original_id  \\\n",
       "0  633922aad8cd96c5e3812afebe56cbc4      VERIFIABLE       209856   \n",
       "1  612be5782034160ae0b7d1c21539d740  NOT VERIFIABLE       172464   \n",
       "2  65cc3f98bece69f9d3a724df9cac82d6      VERIFIABLE       126982   \n",
       "3  1aa8074ed9e9c0e067ffe4e8549ba980      VERIFIABLE        75871   \n",
       "4  77fd25cfc2a6099095b27aeb11a6d086  NOT VERIFIABLE       135082   \n",
       "\n",
       "                                        model_answer  entail_prob  \\\n",
       "0  \\nUse ONLY the text below to answer the claim....     0.464638   \n",
       "1  \\nUse ONLY the text below to answer the claim....     0.000000   \n",
       "2  \\nUse ONLY the text below to answer the claim....     0.000000   \n",
       "3  \\nUse ONLY the text below to answer the claim....     0.000000   \n",
       "4  \\nUse ONLY the text below to answer the claim....     0.000000   \n",
       "\n",
       "   contradict_prob  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.962979  \n",
       "3         0.000000  \n",
       "4         0.000000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../results/final_scored.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3fd2aa4-581c-49a9-955e-0e135107f1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>original_id</th>\n",
       "      <th>model_answer</th>\n",
       "      <th>entail_prob</th>\n",
       "      <th>contradict_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tie Your Mother Down was released in 2007.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Tie_Your_Mother_Down', '8', \"On several occ...</td>\n",
       "      <td>633922aad8cd96c5e3812afebe56cbc4</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>209856</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.464638</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matteo Renzi is Italian and was born in Florence.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...</td>\n",
       "      <td>612be5782034160ae0b7d1c21539d740</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>172464</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashley Graham was never on a magazine cover.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...</td>\n",
       "      <td>65cc3f98bece69f9d3a724df9cac82d6</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>126982</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.962979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luis Fonsi is Puerto Rican.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...</td>\n",
       "      <td>1aa8074ed9e9c0e067ffe4e8549ba980</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>75871</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murda Beatz is a vegan.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Coke_N_Butter', '2', 'The project consists ...</td>\n",
       "      <td>77fd25cfc2a6099095b27aeb11a6d086</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>135082</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim            label  \\\n",
       "0         Tie Your Mother Down was released in 2007.          REFUTES   \n",
       "1  Matteo Renzi is Italian and was born in Florence.  NOT ENOUGH INFO   \n",
       "2       Ashley Graham was never on a magazine cover.          REFUTES   \n",
       "3                        Luis Fonsi is Puerto Rican.         SUPPORTS   \n",
       "4                            Murda Beatz is a vegan.  NOT ENOUGH INFO   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  [['Tie_Your_Mother_Down', '8', \"On several occ...   \n",
       "1  [['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...   \n",
       "2  [['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...   \n",
       "3  [['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...   \n",
       "4  [['Coke_N_Butter', '2', 'The project consists ...   \n",
       "\n",
       "                                 id      verifiable  original_id  \\\n",
       "0  633922aad8cd96c5e3812afebe56cbc4      VERIFIABLE       209856   \n",
       "1  612be5782034160ae0b7d1c21539d740  NOT VERIFIABLE       172464   \n",
       "2  65cc3f98bece69f9d3a724df9cac82d6      VERIFIABLE       126982   \n",
       "3  1aa8074ed9e9c0e067ffe4e8549ba980      VERIFIABLE        75871   \n",
       "4  77fd25cfc2a6099095b27aeb11a6d086  NOT VERIFIABLE       135082   \n",
       "\n",
       "                                        model_answer  entail_prob  \\\n",
       "0  \\nUse ONLY the text below to answer the claim....     0.464638   \n",
       "1  \\nUse ONLY the text below to answer the claim....     0.000000   \n",
       "2  \\nUse ONLY the text below to answer the claim....     0.000000   \n",
       "3  \\nUse ONLY the text below to answer the claim....     0.000000   \n",
       "4  \\nUse ONLY the text below to answer the claim....     0.000000   \n",
       "\n",
       "   contradict_prob  \n",
       "0         0.000000  \n",
       "1         0.000000  \n",
       "2         0.962979  \n",
       "3         0.000000  \n",
       "4         0.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/final_scored.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5eee14c-8b76-4abe-a02b-15dfe0246e23",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'faithfulness'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'faithfulness'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m plt.hist(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfaithfulness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, bins=\u001b[32m15\u001b[39m)\n\u001b[32m      4\u001b[39m plt.title(\u001b[33m\"\u001b[39m\u001b[33mFaithfulness Score Distribution\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mScore\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'faithfulness'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df[\"faithfulness\"], bins=15)\n",
    "plt.title(\"Faithfulness Score Distribution\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9776fadc-2037-4697-ba1e-ad1b9546ed0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer', 'entail_prob', 'contradict_prob'],\n",
      "      dtype='str')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecf82ee5-2f15-462f-a0e1-20dbf974f990",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'similarity'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'similarity'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m\"\u001b[39m] = (\n\u001b[32m      2\u001b[39m     \u001b[32m0.6\u001b[39m * df[\u001b[33m\"\u001b[39m\u001b[33mentail_prob\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     + \u001b[32m0.4\u001b[39m * \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msimilarity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      4\u001b[39m     - \u001b[32m0.3\u001b[39m * df[\u001b[33m\"\u001b[39m\u001b[33mcontradict_prob\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      5\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'similarity'"
     ]
    }
   ],
   "source": [
    "df[\"faithfulness\"] = (\n",
    "    0.6 * df[\"entail_prob\"]\n",
    "    + 0.4 * df[\"similarity\"]\n",
    "    - 0.3 * df[\"contradict_prob\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d316a774-ec8d-4db9-86b5-e99982733a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b45c9bedae47ca81a78a0784f51966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity column added ✅\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sim_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sims = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    emb_ans = sim_model.encode(row[\"model_answer\"], convert_to_tensor=True)\n",
    "    emb_ev = sim_model.encode(row[\"evidence\"], convert_to_tensor=True)\n",
    "    sims.append(util.cos_sim(emb_ans, emb_ev).item())\n",
    "\n",
    "df[\"similarity\"] = sims\n",
    "\n",
    "print(\"similarity column added ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93429126-cfef-4c6f-8bcb-d12d050c92f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faithfulness column added ✅\n"
     ]
    }
   ],
   "source": [
    "df[\"faithfulness\"] = (\n",
    "    0.6 * df[\"entail_prob\"]\n",
    "    + 0.4 * df[\"similarity\"]\n",
    "    - 0.3 * df[\"contradict_prob\"]\n",
    ")\n",
    "\n",
    "print(\"faithfulness column added ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64342737-7f62-43d7-8e95-6fe0e2650d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated final_scored.csv saved 💾\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"../results/final_scored.csv\", index=False)\n",
    "print(\"Updated final_scored.csv saved 💾\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b8feb3b-b39f-476c-9333-a2d5dbf92a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer', 'entail_prob', 'contradict_prob', 'similarity',\n",
      "       'faithfulness'],\n",
      "      dtype='str')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47f82e68-ae34-4efe-a63f-44f0ae2ec98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[\"faithfulness\"], bins=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09b442a5-f00b-401f-a5f8-999b5fb18855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>is_supported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_supported</th>\n",
       "      <td>0.458462</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              faithfulness  is_supported\n",
       "faithfulness      1.000000      0.458462\n",
       "is_supported      0.458462      1.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"is_supported\"] = (df[\"label\"] == \"SUPPORTS\").astype(int)\n",
    "\n",
    "df[[\"faithfulness\", \"is_supported\"]].corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70df321c-60b5-476e-92cb-5f5b7b7203bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.35\n",
    "\n",
    "df[\"pred_supported\"] = (df[\"faithfulness\"] > threshold).astype(int)\n",
    "\n",
    "accuracy = (df[\"pred_supported\"] == df[\"is_supported\"]).mean()\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e71f0a74-2a8e-40af-a488-a28e43fec137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaea269564934df1bebdc3786b42eeaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrida\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hrida\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c124cc71414e98bd1bc0672e8037e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f723ede3ec44a7097d771a3074c0f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340be4dfd26c48d5aaaee086c3180176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'PhiConfig' object has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m generator = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/phi-2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPhi-2 loaded ✅\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:836\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    835\u001b[39m     model_classes = targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m     model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:232\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model, config, model_classes, task, **model_kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m kwargs = model_kwargs.copy()\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# Stop loading on the first successful load.\u001b[39;00m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:372\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    371\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4072\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4069\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4070\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4071\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4072\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# replace module with quantized modules (does not touch weights)\u001b[39;00m\n\u001b[32m   4075\u001b[39m         hf_quantizer.preprocess_model(\n\u001b[32m   4076\u001b[39m             model=model,\n\u001b[32m   4077\u001b[39m             dtype=dtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4080\u001b[39m             use_kernels=use_kernels,\n\u001b[32m   4081\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:468\u001b[39m, in \u001b[36mPhiForCausalLM.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mPhiModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:348\u001b[39m, in \u001b[36mPhiModel.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: PhiConfig):\n\u001b[32m    347\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28mself\u001b[39m.padding_idx = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    351\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, \u001b[38;5;28mself\u001b[39m.padding_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:164\u001b[39m, in \u001b[36mPreTrainedConfig.__getattribute__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    163\u001b[39m     key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PhiConfig' object has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/phi-2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Phi-2 loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "673a2cdd-264a-4b9c-ac4e-4a1edb794f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hrida\\miniconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\hrida\\miniconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: torch in c:\\users\\hrida\\miniconda3\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from typer-slim->transformers) (8.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers accelerate torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d917c10-71f1-464e-bd71-0147a0f34bf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PhiConfig' object has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m generator = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/phi-2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPhi-2 loaded ✅\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:836\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    835\u001b[39m     model_classes = targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m     model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    845\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m    847\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:232\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model, config, model_classes, task, **model_kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m kwargs = model_kwargs.copy()\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# Stop loading on the first successful load.\u001b[39;00m\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:372\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    371\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4072\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4069\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4070\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4071\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4072\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# replace module with quantized modules (does not touch weights)\u001b[39;00m\n\u001b[32m   4075\u001b[39m         hf_quantizer.preprocess_model(\n\u001b[32m   4076\u001b[39m             model=model,\n\u001b[32m   4077\u001b[39m             dtype=dtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4080\u001b[39m             use_kernels=use_kernels,\n\u001b[32m   4081\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:468\u001b[39m, in \u001b[36mPhiForCausalLM.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mPhiModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:348\u001b[39m, in \u001b[36mPhiModel.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: PhiConfig):\n\u001b[32m    347\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28mself\u001b[39m.padding_idx = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    351\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, \u001b[38;5;28mself\u001b[39m.padding_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:164\u001b[39m, in \u001b[36mPreTrainedConfig.__getattribute__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    163\u001b[39m     key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PhiConfig' object has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"microsoft/phi-2\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Phi-2 loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ee3228-8265-4380-900e-e2e8c6f56ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec45d083a3c45209d21ae5ff5f78ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrida\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hrida\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88aa362d64fc4fee82dc61fd44e7c770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce21892828f74330b9de2a1fb497ba41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3995b080d2d44df89905a3ce4c848fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PhiConfig' object has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# IMPORTANT: set padding token\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:372\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    371\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4072\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4069\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4070\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4071\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4072\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# replace module with quantized modules (does not touch weights)\u001b[39;00m\n\u001b[32m   4075\u001b[39m         hf_quantizer.preprocess_model(\n\u001b[32m   4076\u001b[39m             model=model,\n\u001b[32m   4077\u001b[39m             dtype=dtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4080\u001b[39m             use_kernels=use_kernels,\n\u001b[32m   4081\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:468\u001b[39m, in \u001b[36mPhiForCausalLM.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mPhiModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:348\u001b[39m, in \u001b[36mPhiModel.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: PhiConfig):\n\u001b[32m    347\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28mself\u001b[39m.padding_idx = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    351\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, \u001b[38;5;28mself\u001b[39m.padding_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:164\u001b[39m, in \u001b[36mPreTrainedConfig.__getattribute__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    163\u001b[39m     key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PhiConfig' object has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "# IMPORTANT: set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Phi-2 loaded successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61fa7b26-e184-4695-b598-3c102daf783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phi(prompt):\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=120,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24f7fd57-335b-4669-a3a4-5578700a6fb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mevidence\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n\u001b[32m      3\u001b[39m answers_phi = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m claim \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mclaim\u001b[39m\u001b[33m\"\u001b[39m][:\u001b[32m20\u001b[39m]:\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "docs = df[\"evidence\"].astype(str).tolist()\n",
    "\n",
    "answers_phi = []\n",
    "\n",
    "for claim in df[\"claim\"][:20]:\n",
    "    idxs = retrieve_docs(claim)\n",
    "    context = \" \".join([docs[i] for i in idxs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the text below to answer the claim.\n",
    "\n",
    "TEXT:\n",
    "{context}\n",
    "\n",
    "CLAIM:\n",
    "{claim}\n",
    "\n",
    "Answer whether the claim is true or false and explain briefly.\n",
    "\"\"\"\n",
    "\n",
    "    out = generate_phi(prompt)\n",
    "    answers_phi.append(out)\n",
    "\n",
    "print(\"Generated\", len(answers_phi), \"Phi-2 answers ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b242009-dd9f-4d06-bbc5-18540554d9fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mevidence\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "docs = df[\"evidence\"].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab743b6-cb86-4b7f-87a5-44573dbf5749",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mevidence\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "docs = df[\"evidence\"].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9671b02b-9013-4eb7-8eb7-fa7acbdf5786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df loaded: (500, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>original_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tie Your Mother Down was released in 2007.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Tie_Your_Mother_Down', '8', \"On several occ...</td>\n",
       "      <td>633922aad8cd96c5e3812afebe56cbc4</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>209856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matteo Renzi is Italian and was born in Florence.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...</td>\n",
       "      <td>612be5782034160ae0b7d1c21539d740</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>172464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashley Graham was never on a magazine cover.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...</td>\n",
       "      <td>65cc3f98bece69f9d3a724df9cac82d6</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>126982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luis Fonsi is Puerto Rican.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...</td>\n",
       "      <td>1aa8074ed9e9c0e067ffe4e8549ba980</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>75871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murda Beatz is a vegan.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Coke_N_Butter', '2', 'The project consists ...</td>\n",
       "      <td>77fd25cfc2a6099095b27aeb11a6d086</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>135082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim            label  \\\n",
       "0         Tie Your Mother Down was released in 2007.          REFUTES   \n",
       "1  Matteo Renzi is Italian and was born in Florence.  NOT ENOUGH INFO   \n",
       "2       Ashley Graham was never on a magazine cover.          REFUTES   \n",
       "3                        Luis Fonsi is Puerto Rican.         SUPPORTS   \n",
       "4                            Murda Beatz is a vegan.  NOT ENOUGH INFO   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  [['Tie_Your_Mother_Down', '8', \"On several occ...   \n",
       "1  [['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...   \n",
       "2  [['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...   \n",
       "3  [['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...   \n",
       "4  [['Coke_N_Butter', '2', 'The project consists ...   \n",
       "\n",
       "                                 id      verifiable  original_id  \n",
       "0  633922aad8cd96c5e3812afebe56cbc4      VERIFIABLE       209856  \n",
       "1  612be5782034160ae0b7d1c21539d740  NOT VERIFIABLE       172464  \n",
       "2  65cc3f98bece69f9d3a724df9cac82d6      VERIFIABLE       126982  \n",
       "3  1aa8074ed9e9c0e067ffe4e8549ba980      VERIFIABLE        75871  \n",
       "4  77fd25cfc2a6099095b27aeb11a6d086  NOT VERIFIABLE       135082  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../data/fever_small.csv\")\n",
    "\n",
    "print(\"df loaded:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b743a7b6-128f-48f1-8727-1d27f60961e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df[\"evidence\"].astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03379450-09fb-4ad7-ba32-2183d130cdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f4b2654ca748f5accb658705fc10c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "index = faiss.read_index(\"../retrieval/fever.index\")\n",
    "\n",
    "def retrieve_docs(query, k=3):\n",
    "    q_emb = embedder.encode([query])\n",
    "    _, indices = index.search(np.array(q_emb), k)\n",
    "    return indices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "683fd7c5-fe6a-4ad9-87d3-efb362d095fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs ready: 500\n"
     ]
    }
   ],
   "source": [
    "docs = df[\"evidence\"].astype(str).tolist()\n",
    "print(\"Docs ready:\", len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba0ee9d1-f25e-4092-8705-5cd7df0a691d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PhiConfig' object has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mmicrosoft/phi-2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     15\u001b[39m     tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:372\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    371\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    376\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    378\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\modeling_utils.py:4072\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4069\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4070\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4071\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4072\u001b[39m     model = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4074\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# replace module with quantized modules (does not touch weights)\u001b[39;00m\n\u001b[32m   4075\u001b[39m         hf_quantizer.preprocess_model(\n\u001b[32m   4076\u001b[39m             model=model,\n\u001b[32m   4077\u001b[39m             dtype=dtype,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4080\u001b[39m             use_kernels=use_kernels,\n\u001b[32m   4081\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:468\u001b[39m, in \u001b[36mPhiForCausalLM.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[32m    467\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mPhiModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:348\u001b[39m, in \u001b[36mPhiModel.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: PhiConfig):\n\u001b[32m    347\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(config)\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28mself\u001b[39m.padding_idx = \u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28mself\u001b[39m.vocab_size = config.vocab_size\n\u001b[32m    351\u001b[39m     \u001b[38;5;28mself\u001b[39m.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, \u001b[38;5;28mself\u001b[39m.padding_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\configuration_utils.py:164\u001b[39m, in \u001b[36mPreTrainedConfig.__getattribute__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    163\u001b[39m     key = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mattribute_map\u001b[39m\u001b[33m\"\u001b[39m)[key]\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PhiConfig' object has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Phi-2 loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be9f88c6-bb75-4a16-a8fd-ac1fc76e357a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70813225d5b449c8824d8c27206a87fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrida\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hrida\\.cache\\huggingface\\hub\\models--Qwen--Qwen1.5-1.8B-Chat. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1d17cb450a459d808b444426cb7b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4224ece08e30490d996181fdf412d699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df2544bec574fafa496e2274df5036a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a93da3adea44edb4baf060d05b572f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e019d8b309d4be0a74a8721874134a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ba10b8435f4f1f8c851f401a39b397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d8a905e2b44b708c6bfb7c5c1c8ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen loaded ✅\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"Qwen/Qwen1.5-1.8B-Chat\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Qwen loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb57c535-2ef1-4f98-9163-4728ec5f2c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m out = generator(\u001b[43mprompt\u001b[49m, max_new_tokens=\u001b[32m120\u001b[39m)[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mgenerated_text\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'prompt' is not defined"
     ]
    }
   ],
   "source": [
    "out = generator(prompt, max_new_tokens=120)[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f5b94c8-ef79-43cb-9916-42b994de9f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=120) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 20 Qwen answers ✅\n"
     ]
    }
   ],
   "source": [
    "answers_qwen = []\n",
    "\n",
    "for claim in df[\"claim\"][:20]:   # only 20 for speed\n",
    "    idxs = retrieve_docs(claim)\n",
    "    context = \" \".join([docs[i] for i in idxs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the text below to answer the claim.\n",
    "\n",
    "TEXT:\n",
    "{context}\n",
    "\n",
    "CLAIM:\n",
    "{claim}\n",
    "\n",
    "Answer whether the claim is true or false and explain briefly.\n",
    "\"\"\"\n",
    "\n",
    "    out = generator(prompt, max_new_tokens=120)\n",
    "    answers_qwen.append(out[0][\"generated_text\"])\n",
    "\n",
    "print(\"Generated\", len(answers_qwen), \"Qwen answers ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a64876-56c1-42cb-b380-f8e749e75b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '✅' (U+2705) (681138203.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mGenerated 20 Qwen answers ✅\u001b[39m\n                              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character '✅' (U+2705)\n"
     ]
    }
   ],
   "source": [
    "Generated 20 Qwen answers ✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a74d5c7a-667b-4336-b52e-b2a372576d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved qwen.csv ✅\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.makedirs(\"../results/model_comparison\", exist_ok=True)\n",
    "\n",
    "df_qwen = df.head(len(answers_qwen)).copy()\n",
    "df_qwen[\"model_answer\"] = answers_qwen\n",
    "\n",
    "df_qwen.to_csv(\"../results/model_comparison/qwen.csv\", index=False)\n",
    "\n",
    "print(\"Saved qwen.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dbeba3e-2eb5-4676-ba3a-0c37ebfaf6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>verifiable</th>\n",
       "      <th>original_id</th>\n",
       "      <th>model_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tie Your Mother Down was released in 2007.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Tie_Your_Mother_Down', '8', \"On several occ...</td>\n",
       "      <td>633922aad8cd96c5e3812afebe56cbc4</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>209856</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matteo Renzi is Italian and was born in Florence.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...</td>\n",
       "      <td>612be5782034160ae0b7d1c21539d740</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>172464</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ashley Graham was never on a magazine cover.</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...</td>\n",
       "      <td>65cc3f98bece69f9d3a724df9cac82d6</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>126982</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luis Fonsi is Puerto Rican.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...</td>\n",
       "      <td>1aa8074ed9e9c0e067ffe4e8549ba980</td>\n",
       "      <td>VERIFIABLE</td>\n",
       "      <td>75871</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Murda Beatz is a vegan.</td>\n",
       "      <td>NOT ENOUGH INFO</td>\n",
       "      <td>[['Coke_N_Butter', '2', 'The project consists ...</td>\n",
       "      <td>77fd25cfc2a6099095b27aeb11a6d086</td>\n",
       "      <td>NOT VERIFIABLE</td>\n",
       "      <td>135082</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim            label  \\\n",
       "0         Tie Your Mother Down was released in 2007.          REFUTES   \n",
       "1  Matteo Renzi is Italian and was born in Florence.  NOT ENOUGH INFO   \n",
       "2       Ashley Graham was never on a magazine cover.          REFUTES   \n",
       "3                        Luis Fonsi is Puerto Rican.         SUPPORTS   \n",
       "4                            Murda Beatz is a vegan.  NOT ENOUGH INFO   \n",
       "\n",
       "                                            evidence  \\\n",
       "0  [['Tie_Your_Mother_Down', '8', \"On several occ...   \n",
       "1  [['Renzi_-LRB-surname-RRB-', '22', 'Matteo Ren...   \n",
       "2  [['Ashley_Graham_-LRB-model-RRB-', '6', 'In 20...   \n",
       "3  [['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...   \n",
       "4  [['Coke_N_Butter', '2', 'The project consists ...   \n",
       "\n",
       "                                 id      verifiable  original_id  \\\n",
       "0  633922aad8cd96c5e3812afebe56cbc4      VERIFIABLE       209856   \n",
       "1  612be5782034160ae0b7d1c21539d740  NOT VERIFIABLE       172464   \n",
       "2  65cc3f98bece69f9d3a724df9cac82d6      VERIFIABLE       126982   \n",
       "3  1aa8074ed9e9c0e067ffe4e8549ba980      VERIFIABLE        75871   \n",
       "4  77fd25cfc2a6099095b27aeb11a6d086  NOT VERIFIABLE       135082   \n",
       "\n",
       "                                        model_answer  \n",
       "0  \\nUse ONLY the text below to answer the claim....  \n",
       "1  \\nUse ONLY the text below to answer the claim....  \n",
       "2  \\nUse ONLY the text below to answer the claim....  \n",
       "3  \\nUse ONLY the text below to answer the claim....  \n",
       "4  \\nUse ONLY the text below to answer the claim....  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/model_comparison/qwen.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8670c8c6-ef82-4741-a9f8-73bafda32d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved qwen_scored.csv ✅\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"../results/model_comparison/qwen_scored.csv\", index=False)\n",
    "print(\"Saved qwen_scored.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "179105cd-f0e3-4350-b41b-b7c38d6dc3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama: (np.float64(0.45846211691073324), np.float64(0.8))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['faithfulness'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m corr, acc\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTinyLlama:\u001b[39m\u001b[33m\"\u001b[39m, evaluate(tiny))\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQwen:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqwen\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(df):\n\u001b[32m      7\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mis_supported\u001b[39m\u001b[33m\"\u001b[39m] = (df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]==\u001b[33m\"\u001b[39m\u001b[33mSUPPORTS\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     corr = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfaithfulness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mis_supported\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.corr().iloc[\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m]\n\u001b[32m      9\u001b[39m     acc = ((df[\u001b[33m\"\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m\"\u001b[39m]>\u001b[32m0.35\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m) == df[\u001b[33m\"\u001b[39m\u001b[33mis_supported\u001b[39m\u001b[33m\"\u001b[39m]).mean()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m corr, acc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4384\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4383\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4384\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4386\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6302\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6299\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6300\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6302\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6304\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6306\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6355\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6352\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6354\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6355\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['faithfulness'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tiny = pd.read_csv(\"../results/final_scored.csv\")\n",
    "qwen = pd.read_csv(\"../results/model_comparison/qwen_scored.csv\")\n",
    "\n",
    "def evaluate(df):\n",
    "    df[\"is_supported\"] = (df[\"label\"]==\"SUPPORTS\").astype(int)\n",
    "    corr = df[[\"faithfulness\",\"is_supported\"]].corr().iloc[0,1]\n",
    "    acc = ((df[\"faithfulness\"]>0.35).astype(int) == df[\"is_supported\"]).mean()\n",
    "    return corr, acc\n",
    "\n",
    "print(\"TinyLlama:\", evaluate(tiny))\n",
    "print(\"Qwen:\", evaluate(qwen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19ff39a7-734c-4c94-80a2-99e3c424aa12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer'],\n",
      "      dtype='str')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/model_comparison/qwen.csv\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0aca688-83f2-4d24-8a66-2d0998e6876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer'],\n",
      "      dtype='str')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../results/model_comparison/qwen.csv\")\n",
    "\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c308a78-e7ce-43ce-9674-3848193d00e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92006197ce0142299dbe90fb70107002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification LOAD REPORT from: roberta-large-mnli\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "roberta.pooler.dense.weight | UNEXPECTED |  | \n",
      "roberta.pooler.dense.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI model loaded ✅\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nli = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"roberta-large-mnli\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"NLI model loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5dac8454-131f-4aa5-9689-69b671cb7e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entail_probs = []\n",
    "contradict_probs = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    pair = row[\"evidence\"][:1500] + \" </s></s> \" + row[\"model_answer\"][:500]\n",
    "\n",
    "    out = nli(pair, truncation=True)[0]\n",
    "\n",
    "    if out[\"label\"] == \"ENTAILMENT\":\n",
    "        entail_probs.append(out[\"score\"])\n",
    "        contradict_probs.append(0)\n",
    "    elif out[\"label\"] == \"CONTRADICTION\":\n",
    "        entail_probs.append(0)\n",
    "        contradict_probs.append(out[\"score\"])\n",
    "    else:\n",
    "        entail_probs.append(0)\n",
    "        contradict_probs.append(0)\n",
    "\n",
    "df[\"entail_prob\"] = entail_probs\n",
    "df[\"contradict_prob\"] = contradict_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55c49592-6181-436e-9c0a-2c7c60d69db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca986d7a78584059a3a877a666bb08f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sim_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sims = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    a = sim_model.encode(row[\"model_answer\"], convert_to_tensor=True)\n",
    "    e = sim_model.encode(row[\"evidence\"], convert_to_tensor=True)\n",
    "    sims.append(util.cos_sim(a, e).item())\n",
    "\n",
    "df[\"similarity\"] = sims\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "746f7e26-3c41-4c77-8639-827d0fdec516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"faithfulness\"] = (\n",
    "    0.6 * df[\"entail_prob\"]\n",
    "    + 0.4 * df[\"similarity\"]\n",
    "    - 0.3 * df[\"contradict_prob\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7191366c-1a7b-4a9b-882c-f4a7a18d5788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer'],\n",
      "      dtype='str')\n"
     ]
    }
   ],
   "source": [
    "qwen = pd.read_csv(\"../results/model_comparison/qwen_scored.csv\")\n",
    "print(qwen.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "080899f0-36ef-4177-b8bf-ebda6bc2abc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4e60fbba1b4cd48bcdb2a587e47af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification LOAD REPORT from: roberta-large-mnli\n",
      "Key                         | Status     |  | \n",
      "----------------------------+------------+--+-\n",
      "roberta.pooler.dense.weight | UNEXPECTED |  | \n",
      "roberta.pooler.dense.bias   | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLI model loaded ✅\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nli = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"roberta-large-mnli\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"NLI model loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3540d68-2ea9-4ff4-8a84-5fc64a7b9695",
   "metadata": {},
   "outputs": [],
   "source": [
    "entail_probs = []\n",
    "contradict_probs = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    pair = row[\"evidence\"][:1500] + \" </s></s> \" + row[\"model_answer\"][:500]\n",
    "\n",
    "    out = nli(pair, truncation=True)[0]\n",
    "\n",
    "    if out[\"label\"] == \"ENTAILMENT\":\n",
    "        entail_probs.append(out[\"score\"])\n",
    "        contradict_probs.append(0)\n",
    "    elif out[\"label\"] == \"CONTRADICTION\":\n",
    "        entail_probs.append(0)\n",
    "        contradict_probs.append(out[\"score\"])\n",
    "    else:\n",
    "        entail_probs.append(0)\n",
    "        contradict_probs.append(0)\n",
    "\n",
    "df[\"entail_prob\"] = entail_probs\n",
    "df[\"contradict_prob\"] = contradict_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aec3f1ef-5d86-499b-98ab-7eda8b0b6a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182bca97cd524120bb07ff2450412109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "sim_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "sims = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    a = sim_model.encode(row[\"model_answer\"], convert_to_tensor=True)\n",
    "    e = sim_model.encode(row[\"evidence\"], convert_to_tensor=True)\n",
    "    sims.append(util.cos_sim(a, e).item())\n",
    "\n",
    "df[\"similarity\"] = sims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "915bb8a4-7ca5-44d8-9a36-4d4432ce886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"faithfulness\"] = (\n",
    "    0.6 * df[\"entail_prob\"]\n",
    "    + 0.4 * df[\"similarity\"]\n",
    "    - 0.3 * df[\"contradict_prob\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f5e05e4-7cd7-49a6-9121-1fa7ae99be7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved qwen_scored.csv ✅\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"../results/model_comparison/qwen_scored.csv\", index=False)\n",
    "print(\"Saved qwen_scored.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14ed7ae-0ba1-4c99-b7d3-b4d1a01cbdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6d7916d-7687-4633-a256-7e0c3cc02721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer', 'entail_prob', 'contradict_prob', 'similarity',\n",
      "       'faithfulness'],\n",
      "      dtype='str')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a6222ad-c93a-4acc-80c3-ffb26500f667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama: (np.float64(0.45846211691073324), np.float64(0.8))\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['faithfulness'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTinyLlama:\u001b[39m\u001b[33m\"\u001b[39m, evaluate(tiny))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mQwen:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqwen\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate\u001b[39m(df):\n\u001b[32m      7\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mis_supported\u001b[39m\u001b[33m\"\u001b[39m] = (df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]==\u001b[33m\"\u001b[39m\u001b[33mSUPPORTS\u001b[39m\u001b[33m\"\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     corr = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfaithfulness\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mis_supported\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.corr().iloc[\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m]\n\u001b[32m      9\u001b[39m     acc = ((df[\u001b[33m\"\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m\"\u001b[39m]>\u001b[32m0.35\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m) == df[\u001b[33m\"\u001b[39m\u001b[33mis_supported\u001b[39m\u001b[33m\"\u001b[39m]).mean()\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m corr, acc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4384\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4383\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4384\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4386\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6302\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6299\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6300\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6302\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6304\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6305\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6306\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6355\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6352\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6354\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6355\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['faithfulness'] not in index\""
     ]
    }
   ],
   "source": [
    "print(\"TinyLlama:\", evaluate(tiny))\n",
    "print(\"Qwen:\", evaluate(qwen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7db0282d-84a0-4a7c-a05e-44d56ff8bb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer', 'is_supported'],\n",
      "      dtype='str')\n"
     ]
    }
   ],
   "source": [
    "print(qwen.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d07724a-10ba-40e4-8ea4-675a79010d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen = pd.read_csv(\"../results/model_comparison/qwen_scored.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cc41f1c4-0b5b-42dd-bab7-63e18261e638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['claim', 'label', 'evidence', 'id', 'verifiable', 'original_id',\n",
      "       'model_answer', 'entail_prob', 'contradict_prob', 'similarity',\n",
      "       'faithfulness'],\n",
      "      dtype='str')\n"
     ]
    }
   ],
   "source": [
    "print(qwen.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "390ae9ae-118d-40f8-a688-e1f36a89a092",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tiny_corr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m results = pd.DataFrame({\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mTinyLlama\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mQwen\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCorrelation\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[43mtiny_corr\u001b[49m, qwen_corr],\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m: [tiny_acc, qwen_acc]\n\u001b[32m      7\u001b[39m })\n\u001b[32m      9\u001b[39m results\n",
      "\u001b[31mNameError\u001b[39m: name 'tiny_corr' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"TinyLlama\", \"Qwen\"],\n",
    "    \"Correlation\": [tiny_corr, qwen_corr],\n",
    "    \"Accuracy\": [tiny_acc, qwen_acc]\n",
    "})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f5e3414-4b29-4aea-a3b1-8aec47773c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyLlama: 0.45846211691073324 0.8\n",
      "Qwen: 0.5895686986812538 0.8\n"
     ]
    }
   ],
   "source": [
    "tiny_corr, tiny_acc = evaluate(tiny)\n",
    "qwen_corr, qwen_acc = evaluate(qwen)\n",
    "\n",
    "print(\"TinyLlama:\", tiny_corr, tiny_acc)\n",
    "print(\"Qwen:\", qwen_corr, qwen_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1da9988-351e-4319-b0eb-9fddbd4b305e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Correlation</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TinyLlama</td>\n",
       "      <td>0.458462</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>0.589569</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  Correlation  Accuracy\n",
       "0  TinyLlama     0.458462       0.8\n",
       "1       Qwen     0.589569       0.8"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"TinyLlama\", \"Qwen\"],\n",
    "    \"Correlation\": [tiny_corr, qwen_corr],\n",
    "    \"Accuracy\": [tiny_acc, qwen_acc]\n",
    "})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2fbb3c52-312b-4fe5-96c2-85ac5eba7008",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pred_supported'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3641\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3640\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3642\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:168\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:197\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7668\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7676\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'pred_supported'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m mistakes = qwen[\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     (\u001b[43mqwen\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpred_supported\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m == \u001b[32m1\u001b[39m) &\n\u001b[32m      3\u001b[39m     (qwen[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mREFUTES\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m ].head(\u001b[32m5\u001b[39m)\n\u001b[32m      6\u001b[39m mistakes[[\u001b[33m\"\u001b[39m\u001b[33mclaim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mevidence\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel_answer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m\"\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4378\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4380\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3648\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3643\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3644\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3645\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3646\u001b[39m     ):\n\u001b[32m   3647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3648\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3649\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3651\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3652\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3653\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'pred_supported'"
     ]
    }
   ],
   "source": [
    "mistakes = qwen[\n",
    "    (qwen[\"pred_supported\"] == 1) &\n",
    "    (qwen[\"label\"] == \"REFUTES\")\n",
    "].head(5)\n",
    "\n",
    "mistakes[[\"claim\", \"evidence\", \"model_answer\", \"faithfulness\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "032831fb-728d-48c6-9806-5dca368d7d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   faithfulness  pred_supported  is_supported\n",
      "0      0.240246               0             0\n",
      "1      0.181357               0             0\n",
      "2      0.154583               0             0\n",
      "3      0.286129               0             1\n",
      "4      0.185533               0             0\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.35\n",
    "\n",
    "qwen[\"is_supported\"] = (qwen[\"label\"] == \"SUPPORTS\").astype(int)\n",
    "qwen[\"pred_supported\"] = (qwen[\"faithfulness\"] > threshold).astype(int)\n",
    "\n",
    "print(qwen[[\"faithfulness\", \"pred_supported\", \"is_supported\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63f22d51-1850-4834-b992-9837630d1286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>model_answer</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [claim, evidence, model_answer, faithfulness]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes = qwen[\n",
    "    (qwen[\"pred_supported\"] == 1) &\n",
    "    (qwen[\"label\"] == \"REFUTES\")\n",
    "].head(5)\n",
    "\n",
    "mistakes[[\"claim\", \"evidence\", \"model_answer\", \"faithfulness\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "05eede1b-e823-48f4-8c5a-7c1751251173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>model_answer</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [claim, evidence, model_answer, faithfulness]\n",
       "Index: []"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes = qwen[\n",
    "    (qwen[\"pred_supported\"] == 1) &\n",
    "    (qwen[\"label\"] == \"REFUTES\")\n",
    "].head(5)\n",
    "\n",
    "mistakes[[\"claim\", \"evidence\", \"model_answer\", \"faithfulness\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "edb1351a-1156-4eaf-854a-5be75d4a70ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>model_answer</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Luis Fonsi is Puerto Rican.</td>\n",
       "      <td>[['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.286129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Drake Bell released A Reminder in 2011.</td>\n",
       "      <td>[['Drake_Bell', '18', 'Bell released an EP in ...</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.268534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>House is a dramatic work focused on medicine.</td>\n",
       "      <td>[['House_-LRB-TV_series-RRB-', '0', 'House -LR...</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.317134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Smriti Mandhana is an Indian cricketer born in...</td>\n",
       "      <td>[['Smriti_Mandhana', '0', \"Smriti Mandhana -LR...</td>\n",
       "      <td>\\nUse ONLY the text below to answer the claim....</td>\n",
       "      <td>0.312267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                claim  \\\n",
       "3                         Luis Fonsi is Puerto Rican.   \n",
       "12            Drake Bell released A Reminder in 2011.   \n",
       "14      House is a dramatic work focused on medicine.   \n",
       "16  Smriti Mandhana is an Indian cricketer born in...   \n",
       "\n",
       "                                             evidence  \\\n",
       "3   [['Luis_Fonsi', '0', 'Luis Alfonso Rodríguez L...   \n",
       "12  [['Drake_Bell', '18', 'Bell released an EP in ...   \n",
       "14  [['House_-LRB-TV_series-RRB-', '0', 'House -LR...   \n",
       "16  [['Smriti_Mandhana', '0', \"Smriti Mandhana -LR...   \n",
       "\n",
       "                                         model_answer  faithfulness  \n",
       "3   \\nUse ONLY the text below to answer the claim....      0.286129  \n",
       "12  \\nUse ONLY the text below to answer the claim....      0.268534  \n",
       "14  \\nUse ONLY the text below to answer the claim....      0.317134  \n",
       "16  \\nUse ONLY the text below to answer the claim....      0.312267  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = qwen[\n",
    "    (qwen[\"pred_supported\"] == 0) &\n",
    "    (qwen[\"label\"] == \"SUPPORTS\")\n",
    "].head(5)\n",
    "\n",
    "fn[[\"claim\", \"evidence\", \"model_answer\", \"faithfulness\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94f93b5d-60a4-4646-b01c-3def3a5658ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pred_supported\n",
       " 0    19\n",
       " 1     1\n",
       " Name: count, dtype: int64,\n",
       " label\n",
       " REFUTES            8\n",
       " NOT ENOUGH INFO    7\n",
       " SUPPORTS           5\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwen[\"pred_supported\"].value_counts(), qwen[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "64a96677-505a-4c6e-9c48-2a63c1d25153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen AUC: 0.9066666666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, _ = roc_curve(qwen[\"is_supported\"], qwen[\"faithfulness\"])\n",
    "auc_val = auc(fpr, tpr)\n",
    "\n",
    "print(\"Qwen AUC:\", auc_val)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – Qwen\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f710da33-6782-4aad-b33f-6c714f235806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.26530612244897955), np.float64(0.85))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "thresholds = np.linspace(0,1,50)\n",
    "accs = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (qwen[\"faithfulness\"] > t).astype(int)\n",
    "    accs.append((preds == qwen[\"is_supported\"]).mean())\n",
    "\n",
    "best_t = thresholds[np.argmax(accs)]\n",
    "best_acc = max(accs)\n",
    "\n",
    "best_t, best_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "210960bb-fac1-47a3-9e4d-cc936a935c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(thresholds, accs)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Faithfulness Threshold (Qwen)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974566dc-dc21-4137-91a2-ad81a0c4d7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

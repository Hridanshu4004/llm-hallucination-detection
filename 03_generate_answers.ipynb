{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60ed690f-c119-4256-9a2b-db4cfa1b724f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db88602f256f46989b2e04884a7137d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "df = pd.read_csv(\"../data/fever_small.csv\")\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "index = faiss.read_index(\"../retrieval/fever.index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3515bd-aa23-4b29-aee2-20b0b4965659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(query, k=3):\n",
    "    q_emb = embedder.encode([query])\n",
    "    distances, indices = index.search(np.array(q_emb), k)\n",
    "    return indices[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec19bc1-08ad-49d3-b671-897783897c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf783f210a7496db1d324d6006c180c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrida\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hrida\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55e8b0b78e0400f933f6d7eed89791e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77fdd93c17a459e845e8c11cbd972a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/282 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b026df423a45e9b60533407084cbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8d619068834e6eacbce9022a0dc2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568f27d98c7c4950873a74990d1014f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756a7e19c85642e487f40fafc712b9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce17a384d274122ad33be1e4591e839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AfmoeForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'CwmForCausalLM', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'Glm4MoeLiteForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'Jais2ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxM2ForCausalLM', 'MinistralForCausalLM', 'Ministral3ForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NanoChatForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'SolarOpenForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0654c56-b8a2-4af9-a058-2aef7e3a9dd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task text2text-generation, available tasks are ['any-to-any', 'audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'token-classification', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m generator = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext2text-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/flan-t5-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:777\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    770\u001b[39m         pipeline_class = get_class_from_dynamic_module(\n\u001b[32m    771\u001b[39m             class_ref,\n\u001b[32m    772\u001b[39m             model,\n\u001b[32m    773\u001b[39m             code_revision=code_revision,\n\u001b[32m    774\u001b[39m             **hub_kwargs,\n\u001b[32m    775\u001b[39m         )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m777\u001b[39m     normalized_task, targeted_task, task_options = \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    779\u001b[39m         pipeline_class = targeted_task[\u001b[33m\"\u001b[39m\u001b[33mimpl\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:381\u001b[39m, in \u001b[36mcheck_task\u001b[39m\u001b[34m(task)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m, Any]:\n\u001b[32m    342\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[33;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[33;03m    default models if they exist.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    379\u001b[39m \n\u001b[32m    380\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1356\u001b[39m, in \u001b[36mPipelineRegistry.check_task\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m   1353\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[32m1\u001b[39m], tokens[\u001b[32m3\u001b[39m])\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, use \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtranslation_XX_to_YY\u001b[39m\u001b[33m'\u001b[39m\u001b[33m format\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   1357\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.get_supported_tasks()\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m[\u001b[33m'\u001b[39m\u001b[33mtranslation_XX_to_YY\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1358\u001b[39m )\n",
      "\u001b[31mKeyError\u001b[39m: \"Unknown task text2text-generation, available tasks are ['any-to-any', 'audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'token-classification', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5208bcc-05b2-4c2b-aa72-40adafbf1b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\hrida\\miniconda3\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\hrida\\miniconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from accelerate) (2.10.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\hrida\\miniconda3\\lib\\site-packages (from typer-slim->transformers) (8.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "481e973f-3aec-4029-ae2b-9c1abfb5a40b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task text2text-generation, available tasks are ['any-to-any', 'audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'token-classification', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m generator = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext2text-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/flan-t5-base\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:777\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    770\u001b[39m         pipeline_class = get_class_from_dynamic_module(\n\u001b[32m    771\u001b[39m             class_ref,\n\u001b[32m    772\u001b[39m             model,\n\u001b[32m    773\u001b[39m             code_revision=code_revision,\n\u001b[32m    774\u001b[39m             **hub_kwargs,\n\u001b[32m    775\u001b[39m         )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m777\u001b[39m     normalized_task, targeted_task, task_options = \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    779\u001b[39m         pipeline_class = targeted_task[\u001b[33m\"\u001b[39m\u001b[33mimpl\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:381\u001b[39m, in \u001b[36mcheck_task\u001b[39m\u001b[34m(task)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m, Any]:\n\u001b[32m    342\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[33;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[33;03m    default models if they exist.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    379\u001b[39m \n\u001b[32m    380\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:1356\u001b[39m, in \u001b[36mPipelineRegistry.check_task\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m   1353\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[32m1\u001b[39m], tokens[\u001b[32m3\u001b[39m])\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, use \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtranslation_XX_to_YY\u001b[39m\u001b[33m'\u001b[39m\u001b[33m format\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1356\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m   1357\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.get_supported_tasks()\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m[\u001b[33m'\u001b[39m\u001b[33mtranslation_XX_to_YY\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1358\u001b[39m )\n",
      "\u001b[31mKeyError\u001b[39m: \"Unknown task text2text-generation, available tasks are ['any-to-any', 'audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'keypoint-matching', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'token-classification', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0adaef6c-c64a-47fc-baee-95b0b8fe47c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed1a1048cbd493b94412f77305f60db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hrida\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hrida\\.cache\\huggingface\\hub\\models--TinyLlama--TinyLlama-1.1B-Chat-v1.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3b410a648e478f932cacab3db24168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2316c115e5a641e9b02efa657623a5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f7790396e4b492f80737abce38c487c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd6eb8eed224ed8a257092782bf7584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c939b09a25cd42b2a1c6b50f43af05a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3c582b3c274361b6ddf1d6d71d007f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7fa307f7394fda80c91e6b5a83f8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fec32c9-b453-47c7-a7c8-81a43bca1229",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m docs = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mevidence\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).tolist()\n\u001b[32m      3\u001b[39m answers = []\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m claim \u001b[38;5;129;01min\u001b[39;00m df[\u001b[33m\"\u001b[39m\u001b[33mclaim\u001b[39m\u001b[33m\"\u001b[39m][:\u001b[32m50\u001b[39m]:\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "docs = df[\"evidence\"].astype(str).tolist()\n",
    "\n",
    "answers = []\n",
    "\n",
    "for claim in df[\"claim\"][:50]:\n",
    "    idxs = retrieve_docs(claim)\n",
    "    context = \" \".join([docs[i] for i in idxs])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use ONLY the text below to answer the claim.\n",
    "\n",
    "TEXT:\n",
    "{context}\n",
    "\n",
    "CLAIM:\n",
    "{claim}\n",
    "\n",
    "Answer whether the claim is true or false and explain briefly.\n",
    "\"\"\"\n",
    "\n",
    "    out = generator(prompt, max_new_tokens=120)\n",
    "    answers.append(out[0][\"generated_text\"])\n",
    "\n",
    "print(\"Generated\", len(answers), \"answers âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a896da5-9134-4536-81ef-4c79470b1363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
